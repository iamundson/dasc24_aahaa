We evaluated 3 State-Of-The-Art (SOTA) neural network (NN) verifiers: $\alpha$-$\beta$-CROWN, Marabou, and Venus2. We apply the tools to formally verify the properties of 4 Collins neural network applications. A property is an assertion about the mathematical characteristic of a neural network, often in forms of relationship of input and output relationship.

$\alpha$-$\beta$-CROWN reduces the property verification to prove or disprove whether the  outputs of a NN are always non-negative, given the input bounds. The NN is created by adding an linear output layer to the original NN, which models the property. It uses abstract interpretation techniques to efficiently propagate linear bound of neuron values through a NN. Its branch-and-bound approach enables parallelization and utilizes GPUs to accelerate its performance. It has been ranked as the top NN verifier in recent International Verification of Neural Networks Competition, VNN-COMP. It supports many forms of activation functions (e.g., ReLU, tanh, sigmoid).
It supports the verification of a property that is encoded as linear constraints of NN outputs.

Marabou is based on satisfiability modulo theories (SMT). It transforms a NN property verification problem to a satisfiability (SAT) instance, which consists of a set of linear and non-linear constraints (the activation functions) on the neuron values. The property is falsified if and only if  there exists an assignment of neuron values that satisfies all the constraints. Marabou supports NNs with piecewise linear activation functions. It accepts properties that are encoded as linear constraints of both inputs and outputs.

Venus2 formulates a NN verification problem as a Mixed-Integer-Linear-Program (MILP). The property is falsified if and only if the corresponding program is feasible. The MILP can be transformed to a linear program that can be solved in polynomial-time. Venus2 leverages the efficient, Commercial-Off-The-Shelf (COTS) linear program solver, Gurobi. It verifies NNs that only use piecewise linear activation function. It requires the property to be encoded as linear constraints of NN outputs.